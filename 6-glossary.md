# Glossary

| Term        | Explanation                                                                                                                                                                                                                                                                                                |
| ------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Data deserts**         | Areas or groups with little to no data. For example, if a loan approval model is trained mostly on urban credit records, rural borrowers will have almost no representation. The system might reject them or treat them as risky, not because they are risky, but because the data about them is missing. |
| **Proxy variable**       | Something that secretly stands in for something else. For example, ZIP code often correlates with race because of historical segregation. Even if you don’t collect race, your model could still learn racial patterns through ZIP code.                                                                   |
| **Redlining**            | A practice (common in the mid-20th century U.S.) where banks refused loans to people in neighborhoods marked on a map, usually Black, immigrant, or low-income areas, regardless of individual qualifications. Outlawed today, but its effects remain in financial data.                                 |
| **Feedback loop**        | When a model’s decisions influence the data it will later learn from. Example: if an AI denies credit to a group, they never build repayment histories, which makes future models think they’re “risky,” reinforcing the bias.                                                                             |
| **Structural injustice** | Long-standing social or economic inequalities built into systems, like education gaps, wealth gaps, or unequal job access, that can silently flow into AI training data.                                                                                                                                 |
| **Intersectionality**    | When someone belongs to multiple marginalized groups at once (e.g., Black women in low-income jobs). These overlapping identities can face unique disadvantages that single-category analyses may miss.                                                                                                    |
| **True Positive (TP)**  | The system **correctly predicts a positive outcome**. Example: An AI loan model approves a qualified applicant.                     |
| **False Positive (FP)** | The system **incorrectly predicts a positive outcome**. Example: The model approves an applicant who later defaults on the loan.    |
| **True Negative (TN)**  | The system **correctly predicts a negative outcome**. Example: The model rejects an applicant who indeed would have defaulted.      |
| **False Negative (FN)** | The system **incorrectly predicts a negative outcome**. Example: The model rejects an applicant who would have repaid successfully. |                                       |
| **Confidence Interval (CI)** | A range around a metric estimate showing where the “true value” likely falls. If your hiring model shows a 10% gender gap with a ±2% CI, the real gap is probably between 8–12%.                                                 |
| **Credible Interval**        | Like a confidence interval, but from Bayesian statistics. Especially useful when you have very little data (e.g., fewer than 100 examples per group).                                                                            |
| **Bootstrap Resampling**     | A statistical method that repeatedly samples your dataset (with replacement) to check how stable your metric is. Helps verify that an observed disparity isn’t just due to random chance.                                        |
| **Threshold Tuning**         | Adjusting decision cutoffs (like an interview score threshold) separately per group to equalize error rates. Must be documented carefully to avoid hidden quotas or legality issues.                                             |
